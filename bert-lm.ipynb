{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lesser-elements",
   "metadata": {},
   "source": [
    "# High-level API using pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-girlfriend",
   "metadata": {},
   "source": [
    "Let us first apply a fill-mask task using huggingface's high-level fill-mask API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honey-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-dbmdz-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Bestimmt kennen Sie einige Funktionen moderner Technik?',\n",
       "  'score': 0.04415518045425415,\n",
       "  'token': 5808,\n",
       "  'token_str': 'Technik'},\n",
       " {'sequence': 'Bestimmt kennen Sie einige Funktionen moderner Software?',\n",
       "  'score': 0.041537947952747345,\n",
       "  'token': 5177,\n",
       "  'token_str': 'Software'},\n",
       " {'sequence': 'Bestimmt kennen Sie einige Funktionen moderner Medien?',\n",
       "  'score': 0.02656109817326069,\n",
       "  'token': 3562,\n",
       "  'token_str': 'Medien'},\n",
       " {'sequence': 'Bestimmt kennen Sie einige Funktionen moderner Art?',\n",
       "  'score': 0.026073254644870758,\n",
       "  'token': 1622,\n",
       "  'token_str': 'Art'},\n",
       " {'sequence': 'Bestimmt kennen Sie einige Funktionen moderner Geräte?',\n",
       "  'score': 0.021198052912950516,\n",
       "  'token': 7612,\n",
       "  'token_str': 'Geräte'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline('fill-mask', model=\"bert-base-german-dbmdz-cased\")\n",
    "fill_mask(\"Bestimmt kennen Sie einige Funktionen moderner [MASK]?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-sudan",
   "metadata": {},
   "source": [
    "# More details using lower-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-violin",
   "metadata": {},
   "source": [
    "Let's now dive into more details how this is done under the hood. First some helper code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dynamic-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import html\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "class Demo:\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self._tokenizer\n",
    "    \n",
    "    def print_variants(self, inputs, masked_index, k=10):\n",
    "        token_ids = inputs[\"input_ids\"][0]\n",
    "        for x in self.suggest_token_ids(inputs, masked_index, k):\n",
    "            parts = [\n",
    "                html.escape(self._tokenizer.decode(token_ids[:masked_index])),\n",
    "                ' <span style=\"color:red;\">',\n",
    "                html.escape(self._tokenizer.decode([x])),\n",
    "                '</span> ',\n",
    "                html.escape(self._tokenizer.decode(token_ids[masked_index + 1:]))]\n",
    "            display(HTML(\"\".join(parts)))\n",
    "    \n",
    "    \n",
    "class MaskedLMDemo(Demo):\n",
    "    def __init__(self, model_name):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self._model.eval()\n",
    "\n",
    "    def suggest_token_ids(self, inputs, masked_index, k=10):\n",
    "        model = self._model\n",
    "        \n",
    "        token_ids = inputs[\"input_ids\"]\n",
    "        assert len(token_ids.shape) == 2\n",
    "        assert token_ids.shape[0] == 1\n",
    "\n",
    "        mask_token_id = self._tokenizer.mask_token_id\n",
    "        masked_token_ids = token_ids.clone().detach()\n",
    "        masked_token_ids[0, masked_index] = mask_token_id\n",
    "\n",
    "        args = inputs.copy()\n",
    "        args[\"input_ids\"] = masked_token_ids\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**args)\n",
    "\n",
    "        logits = outputs.logits[0]\n",
    "        predictions = logits[masked_index]\n",
    "        softmax = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "        \n",
    "        values, indices = torch.topk(softmax, k)\n",
    "        return indices\n",
    "    \n",
    "    \n",
    "class CausalLMDemo(Demo):\n",
    "    def __init__(self, model_name):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, is_decoder=True)\n",
    "        self._model.eval()\n",
    "\n",
    "    def suggest_token_ids(self, inputs, masked_index, k=10):\n",
    "        model = self._model\n",
    "        \n",
    "        token_ids = inputs[\"input_ids\"]\n",
    "        assert len(token_ids.shape) == 2\n",
    "        assert token_ids.shape[0] == 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=token_ids)\n",
    "            \n",
    "        logits = outputs.logits[0]\n",
    "        predictions = logits[masked_index]\n",
    "        softmax = torch.nn.functional.softmax(predictions, dim=-1)\n",
    "        \n",
    "        values, indices = torch.topk(softmax, k)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-planning",
   "metadata": {},
   "source": [
    "# Masked Language Modelling using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dirty-problem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-dbmdz-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# here we use the German BERT model from the Bavarian State Library, see\n",
    "# https://huggingface.co/dbmdz/bert-base-german-cased\n",
    "\n",
    "# you might also want to try:\n",
    "# * dbmdz/bert-base-german-europeana-cased\n",
    "# * dbmdz/distilbert-base-german-europeana-cased\n",
    "\n",
    "# see https://huggingface.co/dbmdz/bert-base-german-europeana-cased and\n",
    "# https://huggingface.co/dbmdz/distilbert-base-german-europeana-cased\n",
    "\n",
    "demo_mask = MaskedLMDemo(\"bert-base-german-dbmdz-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sharp-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "faust_quote = \"\"\"\n",
    "Habe nun, ach! Philosophie,\n",
    "Juristerei and Medizin,\n",
    "Und leider auch Theologie\n",
    "Durchaus studiert, mit heißem Bemühn.\n",
    "Da steh' ich nun, ich armer Tor,\n",
    "Und bin so klug als wie zuvor!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-helicopter",
   "metadata": {},
   "source": [
    "Let us first tokenize this input using our BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olympic-rover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  102, 15454,  1269,   818, 23070,  3330,  9186,   818, 19228,  5736,\n",
       "          1257,  6070,   818,   700,  8433,   313, 12603,  1209,   322, 17722,\n",
       "           818,   212, 15996, 30895,  5614,  6018, 30882,   566,   867,  4729,\n",
       "         30889,  2119,   383,  1269,   818,   383, 28983, 30884,  2270,   818,\n",
       "           700,  1089,   262, 29107,   276,   335,  3489,  3330,   103]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_mask = demo_mask.tokenizer(faust_quote, return_tensors=\"pt\")\n",
    "\n",
    "inputs_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-avenue",
   "metadata": {},
   "source": [
    "To actually understand the tokenization, let's convert the token ids into token strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "early-insert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Habe',\n",
       " 'nun',\n",
       " ',',\n",
       " 'ach',\n",
       " '!',\n",
       " 'Philosophie',\n",
       " ',',\n",
       " 'Jurist',\n",
       " '##erei',\n",
       " 'and',\n",
       " 'Medizin',\n",
       " ',',\n",
       " 'Und',\n",
       " 'leider',\n",
       " 'auch',\n",
       " 'Theologie',\n",
       " 'Durch',\n",
       " '##aus',\n",
       " 'studiert',\n",
       " ',',\n",
       " 'mit',\n",
       " 'heiße',\n",
       " '##m',\n",
       " 'Bem',\n",
       " '##üh',\n",
       " '##n',\n",
       " '.',\n",
       " 'Da',\n",
       " 'ste',\n",
       " '##h',\n",
       " \"'\",\n",
       " 'ich',\n",
       " 'nun',\n",
       " ',',\n",
       " 'ich',\n",
       " 'arme',\n",
       " '##r',\n",
       " 'Tor',\n",
       " ',',\n",
       " 'Und',\n",
       " 'bin',\n",
       " 'so',\n",
       " 'klug',\n",
       " 'als',\n",
       " 'wie',\n",
       " 'zuvor',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_mask = demo_mask.tokenizer.convert_ids_to_tokens(\n",
    "    inputs_mask[\"input_ids\"][0])\n",
    "tokens_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-logistics",
   "metadata": {},
   "source": [
    "We can convert the sequence of token ids back into a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bearing-screw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh'ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_mask.tokenizer.decode(inputs_mask[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-camel",
   "metadata": {},
   "source": [
    "Let's now get suggestions for replacing a token. The following call gives us the ids of token that would make sense at one location according to BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-breast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3392,  1269,   704,   742,  2913,  1184,  1202,   494, 30170,  1619])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_mask.suggest_token_ids(inputs_mask, tokens_mask.index(\"leider\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-financing",
   "metadata": {},
   "source": [
    "It is easy to convert these ids into token strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suburban-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natürlich',\n",
       " 'nun',\n",
       " 'habe',\n",
       " 'dann',\n",
       " 'schließlich',\n",
       " 'jetzt',\n",
       " 'eben',\n",
       " 'aber',\n",
       " 'nebenbei',\n",
       " 'ja']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_mask.tokenizer.convert_ids_to_tokens(\n",
    "    demo_mask.suggest_token_ids(inputs_mask, tokens_mask.index(\"leider\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-marks",
   "metadata": {},
   "source": [
    "Now let's use a utility function to replace a token with some other candidates the model suggests and print the result as HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "federal-scoop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und <span style=\"color:red;\">natürlich</span> auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und <span style=\"color:red;\">nun</span> auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und <span style=\"color:red;\">habe</span> auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und <span style=\"color:red;\">dann</span> auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und <span style=\"color:red;\">schließlich</span> auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_mask.print_variants(\n",
    "    inputs_mask, tokens_mask.index(\"leider\"), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "friendly-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus <span style=\"color:red;\">alles</span> , mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus <span style=\"color:red;\">nicht</span> , mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus <span style=\"color:red;\">studiert</span> , mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus <span style=\"color:red;\">gebildet</span> , mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus <span style=\"color:red;\">gut</span> , mit heißem Bemühn. Da steh&#x27;ich nun, ich armer Tor, Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_mask.print_variants(\n",
    "    inputs_mask, tokens_mask.index(\"studiert\"), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "electric-harris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer <span style=\"color:red;\">Mann</span> , Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer <span style=\"color:red;\">Herr</span> , Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer <span style=\"color:red;\">Mensch</span> , Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer <span style=\"color:red;\">Sohn</span> , Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[CLS] Habe nun, ach! Philosophie, Juristerei and Medizin, Und leider auch Theologie Durchaus studiert, mit heißem Bemühn. Da steh&#x27;ich nun, ich armer <span style=\"color:red;\">Kerl</span> , Und bin so klug als wie zuvor! [SEP]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_mask.print_variants(\n",
    "    inputs_mask, tokens_mask.index(\"Tor\"), k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-representative",
   "metadata": {},
   "source": [
    "# Causal LM using GottBERT (a RoBERTa model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "imported-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_causal = CausalLMDemo(\"uklfr/gottbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "known-violence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 51963,  8030,   196,     5, 15398,    73,  5846,     5, 51963,\n",
       "           800,   578,  5433,   671,  1138,  2880,     5, 51963,   904,   747,\n",
       "            25, 14737, 51963,  1925,   690, 11763,     5,    12, 33640, 14133,\n",
       "          1376,  3506,     4, 51963,  1255, 28538,   673,    32,   196,     5,\n",
       "            32,  4875,  1144,  1594,     5, 51963,   904,   209,    55, 24722,\n",
       "            37,    44,  1756,    73, 51963,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_c = demo_causal.tokenizer(faust_quote, return_tensors=\"pt\")\n",
    "\n",
    "inputs_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-amendment",
   "metadata": {},
   "source": [
    "Let us note how the tokenization from this model is different from the first one we used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "motivated-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Ċ',\n",
       " 'Habe',\n",
       " 'Ġnun',\n",
       " ',',\n",
       " 'Ġach',\n",
       " '!',\n",
       " 'ĠPhilosophie',\n",
       " ',',\n",
       " 'Ċ',\n",
       " 'J',\n",
       " 'ur',\n",
       " 'ister',\n",
       " 'ei',\n",
       " 'Ġand',\n",
       " 'ĠMedizin',\n",
       " ',',\n",
       " 'Ċ',\n",
       " 'Und',\n",
       " 'Ġleider',\n",
       " 'Ġauch',\n",
       " 'ĠTheologie',\n",
       " 'Ċ',\n",
       " 'Durch',\n",
       " 'aus',\n",
       " 'Ġstudiert',\n",
       " ',',\n",
       " 'Ġmit',\n",
       " 'ĠheiÃŁem',\n",
       " 'ĠBem',\n",
       " 'Ã¼',\n",
       " 'hn',\n",
       " '.',\n",
       " 'Ċ',\n",
       " 'Da',\n",
       " 'Ġsteh',\n",
       " \"'\",\n",
       " 'Ġich',\n",
       " 'Ġnun',\n",
       " ',',\n",
       " 'Ġich',\n",
       " 'Ġar',\n",
       " 'mer',\n",
       " 'ĠTor',\n",
       " ',',\n",
       " 'Ċ',\n",
       " 'Und',\n",
       " 'Ġbin',\n",
       " 'Ġso',\n",
       " 'Ġklug',\n",
       " 'Ġals',\n",
       " 'Ġwie',\n",
       " 'Ġzuvor',\n",
       " '!',\n",
       " 'Ċ',\n",
       " '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_c = demo_causal.tokenizer.convert_ids_to_tokens(\n",
    "    inputs_c[\"input_ids\"][0])\n",
    "tokens_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "running-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und <span style=\"color:red;\"> leider</span>  auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer Tor,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und <span style=\"color:red;\"> Leider</span>  auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer Tor,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und <span style=\"color:red;\"> sehr</span>  auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer Tor,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und <span style=\"color:red;\"> nur</span>  auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer Tor,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und <span style=\"color:red;\"> schade</span>  auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer Tor,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_causal.print_variants(inputs_c, tokens_c.index(\"Ġleider\"), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stunning-french",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\"> Tor</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\"> tor</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\">tor</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\">Tor</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\">or</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\"> Toren</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\"> Tore</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\"> T</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\"> Tag</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;s&gt;\n",
       "Habe nun, ach! Philosophie,\n",
       "Juristerei and Medizin,\n",
       "Und leider auch Theologie\n",
       "Durchaus studiert, mit heißem Bemühn.\n",
       "Da steh&#x27; ich nun, ich armer <span style=\"color:red;\">stor</span> ,\n",
       "Und bin so klug als wie zuvor!\n",
       "&lt;/s&gt;"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_causal.print_variants(inputs_c, tokens_c.index(\"ĠTor\"), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-wilson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
